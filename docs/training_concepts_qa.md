# AI 모델 학습 개념 Q&A

이 문서는 로또 AI 모델 학습에 대한 질의응답을 정리한 것입니다.

---

## Q1: 학습과거 20개, epoch 200이면 어떻게 동작하나요?

### 데이터 구조 (seq_len=20 기준)

총 1,204회차 데이터가 있다면:

```
샘플 1: 입력 [1~20회차] → 타겟 [21회차]
샘플 2: 입력 [2~21회차] → 타겟 [22회차]
샘플 3: 입력 [3~22회차] → 타겟 [23회차]
...
샘플 1184: 입력 [1184~1203회차] → 타겟 [1204회차]
```

**→ 총 1,184개 학습 샘플 생성**

### Epoch 이란?

```
Epoch 1: 1,184개 샘플 전부 1번씩 학습 (순서 섞어서)
Epoch 2: 1,184개 샘플 전부 1번씩 학습 (다시 섞어서)
...
Epoch 200: 1,184개 샘플 전부 1번씩 학습
```

**→ 총 200 × 1,184 = 236,800번 학습**

### 한 Epoch 내부 동작

```python
# 배치 사이즈 = 32일 때
# 1,184개 샘플 / 32 = 약 37개 배치

for batch in data_loader:  # 37번 반복
    inputs = batch[:, :20, :]   # 과거 20회차
    targets = batch[:, 21]      # 21번째 (예측 대상)
    
    predictions = model(inputs)  # 예측
    loss = calculate_loss(predictions, targets)
    loss.backward()  # 역전파
    optimizer.step()  # 가중치 업데이트
```

---

## Q2: 왜 전체 샘플을 200번 반복 학습하나요?

### 1. 점진적 학습 (Gradient Descent)

```
Epoch 1:  가중치가 랜덤 → 예측 엉터리 → 조금 개선
Epoch 10: 대략적인 패턴 파악 → 더 개선
Epoch 50: 패턴 학습 진행 중 → 더 개선
Epoch 100: 상당히 학습됨 → 미세 조정
Epoch 200: 수렴 (더 이상 개선 X)
```

**→ 한 번에 완벽히 배울 수 없고, 조금씩 개선됨**

### 2. 비유: 시험 공부

```
1회독: 책 전체 훑음 → 뭐가 뭔지 감 잡음
2회독: 다시 읽음 → 이해도 ↑
3회독: 또 읽음 → 세부사항 파악
...
N회독: 다 외움
```

### 3. Loss (손실) 감소 과정

```
Epoch   1: Loss = 3.35  (많이 틀림)
Epoch  20: Loss = 3.20  (개선 중)
Epoch  50: Loss = 3.15  (더 개선)
Epoch 100: Loss = 3.12  (수렴 중)
Epoch 200: Loss = 3.10  (거의 수렴)
```

### 4. 왜 한 번에 다 못 배우나?

```python
# 1회 학습 시 가중치 업데이트
new_weight = old_weight - learning_rate × gradient
                                ↑
                         보통 0.001 (아주 작음)
```

**→ 학습률이 작아서 조금씩만 이동**
**→ 너무 크면 발산 (학습 실패)**

### 5. 과적합 (Overfitting) 주의

```
Epoch 100까지: Train Loss ↓, Val Loss ↓ (좋음 ✅)
Epoch 200까지: Train Loss ↓, Val Loss ↑ (과적합 ❌)
```

**→ Validation Loss가 올라가면 학습 중단해야 함**

---

## Q3: 학습과 Validation으로 데이터를 어떻게 나누나요?

### 현재 구현: 시간순 분할

```python
train_ratio = 0.8  # 80% 학습, 20% 검증

train_indices = list(range(train_size))         # 앞쪽 (오래된)
val_indices = list(range(train_size, total))    # 뒷쪽 (최근)
```

### 시각화

```
전체 1,184개 샘플:

┌───────────────────────────────────┬──────────────┐
│        Train (80%)                │  Val (20%)   │
│       오래된 947개                 │  최근 237개   │
├───────────────────────────────────┼──────────────┤
│ 학습에 사용 (가중치 업데이트 O)    │ 검증만 (X)    │
└───────────────────────────────────┴──────────────┘
```

### 왜 시간순으로 나누나?

**일반 시계열 데이터의 경우:**
```
✅ 시간순: 과거로 학습 → 미래 예측 (실제 상황과 동일)
❌ 랜덤: 미래 데이터로 학습 → 과거 예측 (현실에서 불가능)
```

---

## Q4: 랜덤 파티션이 더 낫지 않나요?

### 두 방식 비교

| 방식 | 장점 | 단점 |
|------|------|------|
| **랜덤 분할** | 데이터 다양성 ↑, 과적합 방지 | 미래 정보 누출 (data leakage) |
| **시간순 분할** | 실제 상황과 동일 | 오래된 패턴에만 학습 |

### 로또의 특수성

로또는 **완전 무작위**이므로:

✅ **랜덤이 나을 수도 있음**
- 어차피 시간 순서에 패턴이 없음
- 더 많은 데이터 조합으로 학습

❌ **시간순의 의미가 약함**
- "과거로 미래 예측"이라는 논리가 로또에선 무의미
- 100회차나 1000회차나 같은 확률

### 결론

**로또에선 랜덤 분할이 더 합리적**일 수 있음.

시간순 분할은 **주가 예측, 날씨 예측** 같은 실제 시계열 의존성이 있는 데이터에 적합.

---

## Q5: 그런데 많은 사람들이 로또가 랜덤이라 안 믿으니...

### 현실적인 관점

```
수학적 사실: 로또 = 완전 무작위
사람들 믿음: "패턴이 있다", "AI가 맞출 수 있다"
               ↓
         마케팅 포인트
```

### 그래서...

| 관점 | 중요한 것 |
|------|----------|
| **학문적** | 정확한 검증 방법론 |
| **마케팅** | "AI가 분석했다"는 스토리 ✨ |

**"Transformer가 분석한 번호"** 라고 하면:
- 사람들: "오 AI가 뽑아줬네" 🤩
- 현실: 가중치 랜덤 😅

**결론: 어차피 재미용이니까, 있어 보이는 게 더 중요!**

---

## 핵심 요약

| 용어 | 의미 |
|------|------|
| `seq_len=20` | 과거 20회차를 입력으로 |
| `샘플` | 입력 20개 + 타겟 1개 |
| `Epoch` | 모든 샘플을 1번씩 학습 |
| `Train` | 가중치 업데이트에 사용 |
| `Validation` | 성능 측정만 (학습 X) |
| `과적합` | 학습 데이터만 외움, 새 데이터에 약함 |
