"""
GAN ë¡œë˜ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
import argparse
import json

from models.gan.gan import create_generator, create_discriminator
from models.gan.dataloader import create_dataloader

CONFIG_PATH = Path(__file__).parent / 'config.json'

def load_config():
    with open(CONFIG_PATH, 'r') as f:
        return json.load(f)


def sample_from_generator(generator, batch_size, device):
    """Generatorì—ì„œ ì†Œí”„íŠ¸ë§¥ìŠ¤ ìƒ˜í”Œë§í•˜ì—¬ ë²ˆí˜¸ ìƒì„±"""
    z = torch.randn(batch_size, generator.latent_dim, device=device)
    logits = generator(z)  # (batch, 6, 45)
    
    # Gumbel-Softmaxë¡œ ë¯¸ë¶„ ê°€ëŠ¥í•œ ìƒ˜í”Œë§
    probs = torch.softmax(logits, dim=-1)
    
    # argmaxë¡œ ë²ˆí˜¸ ì„ íƒ (í•™ìŠµ ì‹œì—ëŠ” soft ë²„ì „ ì‚¬ìš©)
    selected = torch.argmax(probs, dim=-1) + 1  # (batch, 6), 1~45
    
    return selected, logits


def main():
    config = load_config()
    model_cfg = config['model']
    train_cfg = config['training']
    paths_cfg = config['paths']
    
    parser = argparse.ArgumentParser(description='GAN Lotto í•™ìŠµ')
    parser.add_argument('--epochs', type=int, default=train_cfg['epochs'], help='ì—í­ ìˆ˜')
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤
    if torch.backends.mps.is_available():
        device = torch.device('mps')
        print('ğŸ Using Apple MPS')
    elif torch.cuda.is_available():
        device = torch.device('cuda')
        print('ğŸ® Using CUDA')
    else:
        device = torch.device('cpu')
        print('ğŸ’» Using CPU')
    
    # ë°ì´í„° ë¡œë”
    print(f'\nğŸ“Š ë°ì´í„° ë¡œë”©: {paths_cfg["data"]}')
    dataloader = create_dataloader(paths_cfg['data'], train_cfg['batch_size'])
    print(f'   ì´ ìƒ˜í”Œ: {len(dataloader.dataset)}')
    
    # ëª¨ë¸ ìƒì„±
    generator = create_generator(model_cfg).to(device)
    discriminator = create_discriminator(model_cfg).to(device)
    
    g_params = sum(p.numel() for p in generator.parameters())
    d_params = sum(p.numel() for p in discriminator.parameters())
    print(f'\nğŸ¤– Generator íŒŒë¼ë¯¸í„°: {g_params:,}')
    print(f'ğŸ¤– Discriminator íŒŒë¼ë¯¸í„°: {d_params:,}')
    
    # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
    criterion = nn.BCELoss()
    
    optimizer_g = optim.Adam(
        generator.parameters(),
        lr=train_cfg['lr_generator'],
        betas=(train_cfg['beta1'], train_cfg['beta2'])
    )
    optimizer_d = optim.Adam(
        discriminator.parameters(),
        lr=train_cfg['lr_discriminator'],
        betas=(train_cfg['beta1'], train_cfg['beta2'])
    )
    
    # ë ˆì´ë¸”
    real_label = 1.0
    fake_label = 0.0
    
    print(f'\nğŸš€ í•™ìŠµ ì‹œì‘ (ì—í­: {args.epochs})')
    print('-' * 60)
    
    for epoch in range(1, args.epochs + 1):
        g_loss_total = 0
        d_loss_total = 0
        
        for real_numbers in dataloader:
            real_numbers = real_numbers.to(device)
            batch_size = real_numbers.size(0)
            
            # === Discriminator í•™ìŠµ ===
            discriminator.zero_grad()
            
            # ì§„ì§œ ë°ì´í„°
            real_labels = torch.full((batch_size, 1), real_label, device=device)
            real_output = discriminator(real_numbers)
            d_loss_real = criterion(real_output, real_labels)
            
            # ê°€ì§œ ë°ì´í„°
            fake_numbers, _ = sample_from_generator(generator, batch_size, device)
            fake_labels = torch.full((batch_size, 1), fake_label, device=device)
            fake_output = discriminator(fake_numbers.detach())
            d_loss_fake = criterion(fake_output, fake_labels)
            
            d_loss = d_loss_real + d_loss_fake
            d_loss.backward()
            optimizer_d.step()
            
            # === Generator í•™ìŠµ ===
            generator.zero_grad()
            
            fake_numbers, _ = sample_from_generator(generator, batch_size, device)
            fake_output = discriminator(fake_numbers)
            g_loss = criterion(fake_output, real_labels)  # ì§„ì§œë¡œ ì†ì´ë ¤ê³ 
            
            g_loss.backward()
            optimizer_g.step()
            
            g_loss_total += g_loss.item()
            d_loss_total += d_loss.item()
        
        avg_g_loss = g_loss_total / len(dataloader)
        avg_d_loss = d_loss_total / len(dataloader)
        
        if epoch % 10 == 0 or epoch == 1:
            print(f'Epoch {epoch:3d} | G Loss: {avg_g_loss:.4f} | D Loss: {avg_d_loss:.4f}')
            
            # ìƒ˜í”Œ ìƒì„±
            sample = generator.generate(3, device)
            print(f'   ìƒ˜í”Œ: {sample.cpu().tolist()}')
    
    print('-' * 60)
    print('ğŸ‰ í•™ìŠµ ì™„ë£Œ!')
    
    # ëª¨ë¸ ì €ì¥
    torch.save({
        'generator_state_dict': generator.state_dict(),
        'discriminator_state_dict': discriminator.state_dict(),
        'config': model_cfg
    }, paths_cfg['checkpoint_g'])
    print(f'   ì €ì¥: {paths_cfg["checkpoint_g"]}')


if __name__ == '__main__':
    main()
