"""
Transformer λ΅λ λ¨λΈ ν•™μµ μ¤ν¬λ¦½νΈ
config.jsonμ—μ„ νλΌλ―Έν„°λ¥Ό λ΅λ“ν•©λ‹λ‹¤.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
import argparse
import json

from models.transformer.transformer import create_model
from models.transformer.dataloader import create_dataloaders

# Config λ΅λ“
CONFIG_PATH = Path(__file__).parent / 'config.json'

def load_config():
    with open(CONFIG_PATH, 'r') as f:
        return json.load(f)


def train_epoch(model, loader, criterion, optimizer, device):
    """ν• μ—ν­ ν•™μµ"""
    model.train()
    total_loss = 0
    
    for batch_idx, (seq, target) in enumerate(loader):
        seq = seq.to(device)
        target = target.to(device)
        
        optimizer.zero_grad()
        output = model(seq)
        
        loss = 0
        for i in range(6):
            loss += criterion(output[:, i, :], target[:, i])
        loss /= 6
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(loader)


def validate(model, loader, criterion, device):
    """κ²€μ¦"""
    model.eval()
    total_loss = 0
    correct_nums = 0
    total_nums = 0
    
    with torch.no_grad():
        for seq, target in loader:
            seq = seq.to(device)
            target = target.to(device)
            
            output = model(seq)
            
            loss = 0
            for i in range(6):
                loss += criterion(output[:, i, :], target[:, i])
                top10 = output[:, i, :].topk(10, dim=1).indices
                for b in range(target.size(0)):
                    if target[b, i] in top10[b]:
                        correct_nums += 1
                    total_nums += 1
            loss /= 6
            
            total_loss += loss.item()
    
    return total_loss / len(loader), correct_nums / total_nums if total_nums > 0 else 0


def main():
    # Config λ΅λ“
    config = load_config()
    model_cfg = config['model']
    train_cfg = config['training']
    paths_cfg = config['paths']
    
    parser = argparse.ArgumentParser(description='Lotto Transformer ν•™μµ')
    parser.add_argument('--epochs', type=int, default=train_cfg['epochs'], help='μ—ν­ μ')
    parser.add_argument('--batch-size', type=int, default=train_cfg['batch_size'], help='λ°°μΉ ν¬κΈ°')
    parser.add_argument('--lr', type=float, default=train_cfg['learning_rate'], help='ν•™μµλ¥ ')
    args = parser.parse_args()
    
    # λ””λ°”μ΄μ¤ μ„¤μ •
    if torch.backends.mps.is_available():
        device = torch.device('mps')
        print('π Using Apple MPS')
    elif torch.cuda.is_available():
        device = torch.device('cuda')
        print('π® Using CUDA')
    else:
        device = torch.device('cpu')
        print('π’» Using CPU')
    
    # λ°μ΄ν„° λ΅λ”
    print(f'\nπ“ λ°μ΄ν„° λ΅λ”©: {paths_cfg["data"]}')
    train_loader, val_loader = create_dataloaders(
        paths_cfg['data'],
        seq_len=model_cfg['seq_len'],
        batch_size=args.batch_size,
        train_ratio=train_cfg['train_ratio']
    )
    print(f'   ν•™μµ μƒν”: {len(train_loader.dataset)}')
    print(f'   κ²€μ¦ μƒν”: {len(val_loader.dataset)}')
    
    # λ¨λΈ μƒμ„± (configμ—μ„ νλΌλ―Έν„° λ΅λ“)
    model = create_model(model_cfg).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f'\nπ¤– λ¨λΈ νλΌλ―Έν„°: {total_params:,}')
    
    # μ†μ‹¤ ν•¨μ λ° μµν‹°λ§μ΄μ €
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=train_cfg['weight_decay'])
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
    
    # μ²΄ν¬ν¬μΈνΈ λ””λ ‰ν† λ¦¬
    checkpoint_dir = Path(paths_cfg['checkpoint']).parent
    checkpoint_dir.mkdir(exist_ok=True, parents=True)
    
    # ν•™μµ
    print(f'\nπ€ ν•™μµ μ‹μ‘ (μ—ν­: {args.epochs})')
    print('-' * 50)
    
    best_val_loss = float('inf')
    
    for epoch in range(1, args.epochs + 1):
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
        val_loss, val_acc = validate(model, val_loader, criterion, device)
        scheduler.step()
        
        print(f'Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Top-10 Acc: {val_acc:.2%}')
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'config': model_cfg
            }, paths_cfg['checkpoint'])
            print(f'   β… Best model saved!')
    
    print('-' * 50)
    print('π‰ ν•™μµ μ™„λ£!')
    print(f'   μµκ³  κ²€μ¦ μ†μ‹¤: {best_val_loss:.4f}')
    print(f'   μ²΄ν¬ν¬μΈνΈ: {paths_cfg["checkpoint"]}')


if __name__ == '__main__':
    main()
